{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-18T08:40:13.673557Z",
     "start_time": "2025-03-18T08:40:07.969705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在下载 AAPL 数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data for AAPL:\n",
      "Head:\n",
      "Price           Close       High        Low       Open     Volume\n",
      "Ticker           AAPL       AAPL       AAPL       AAPL       AAPL\n",
      "Date                                                             \n",
      "2015-01-02  24.320431  24.789800  23.879980  24.778677  212818400\n",
      "2015-01-05  23.635283  24.169162  23.448426  24.089080  257142000\n",
      "2015-01-06  23.637508  23.897774  23.274914  23.699794  263188400\n",
      "2015-01-07  23.968962  24.069063  23.735389  23.846614  160423600\n",
      "2015-01-08  24.889902  24.947740  24.180287  24.298187  237458000\n",
      "Tail:\n",
      "Price            Close        High         Low        Open    Volume\n",
      "Ticker            AAPL        AAPL        AAPL        AAPL      AAPL\n",
      "Date                                                                \n",
      "2025-03-11  220.839996  225.839996  217.449997  223.809998  76137400\n",
      "2025-03-12  216.979996  221.750000  214.910004  220.139999  62547500\n",
      "2025-03-13  209.679993  216.839996  208.419998  215.949997  61368300\n",
      "2025-03-14  213.490005  213.949997  209.580002  211.250000  60107600\n",
      "2025-03-17  214.000000  215.220001  209.970001  213.309998  48042800\n",
      "\n",
      "所有股票数据已保存到 stock_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def download_stock_data(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    下载每支股票数据，并返回包含股票代码和数据的字典\n",
    "    \"\"\"\n",
    "    stock_data = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            print(f\"正在下载 {ticker} 数据...\")\n",
    "            data = yf.download(ticker, start=start_date, end=end_date)\n",
    "            if data.empty:\n",
    "                print(f\"股票 {ticker} 数据为空，跳过\")\n",
    "                continue\n",
    "            stock_data[ticker] = data\n",
    "            # 暂停 0.1 秒以防请求过快\n",
    "            time.sleep(0.1)\n",
    "        except Exception as e:\n",
    "            print(f\"股票 {ticker} 下载时出现异常：{e}\")\n",
    "    return stock_data\n",
    "\n",
    "# 定义 30 个美股股票代码\n",
    "tickers = [\n",
    "    \"AAPL\"\n",
    "]\n",
    "\n",
    "# 设置时间范围：2022 年至 2025 年（end_date 可能受当前日期限制）\n",
    "start_date = \"2015-01-01\"\n",
    "end_date = \"2025-12-31\"\n",
    "\n",
    "# 下载股票数据\n",
    "stock_data_dict = download_stock_data(tickers, start_date, end_date)\n",
    "\n",
    "# 输出每个股票数据的简单示例，检查前 5 行和后 5 行\n",
    "for ticker, data in stock_data_dict.items():\n",
    "    print(f\"\\nData for {ticker}:\")\n",
    "    print(\"Head:\")\n",
    "    print(data.head())\n",
    "    print(\"Tail:\")\n",
    "    print(data.tail())\n",
    "\n",
    "# 将所有下载的数据合并，并筛选只保留指定的列\n",
    "all_data = []\n",
    "for ticker, data in stock_data_dict.items():\n",
    "    # 重置索引，把日期从行索引转换为列\n",
    "    df = data.reset_index()\n",
    "    # 保留指定的列：Date, Open, High, Low, Close, Volume\n",
    "    # 注意：这里要求字段名称保持一致，所以不进行小写转换\n",
    "    df = df[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "    # 添加股票代码列，并命名为 \"stock_id\"\n",
    "    df[\"stock_id\"] = ticker\n",
    "    all_data.append(df)\n",
    "\n",
    "if not all_data:\n",
    "    print(\"未获取到任何股票数据，退出。\")\n",
    "else:\n",
    "    # 合并所有数据，并按股票及日期排序（可选）\n",
    "    all_stock_data = pd.concat(all_data, ignore_index=True)\n",
    "    all_stock_data = all_stock_data.sort_values([\"stock_id\", \"Date\"]).reset_index(drop=True)\n",
    "    \n",
    "    # 只保留指定的列（确保最终 CSV 列数正确）\n",
    "    all_stock_data = all_stock_data[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"stock_id\"]]\n",
    "    \n",
    "    csv_filename = \"stock_data.csv\"\n",
    "    all_stock_data.to_csv(csv_filename, index=False)\n",
    "    print(f\"\\n所有股票数据已保存到 {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据总长度: 2566, 训练截止点: 2486\n",
      "验证集数量: 80\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, GroupNormalizer\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.models.deepar import DeepAR\n",
    "from pytorch_forecasting.metrics import NormalDistributionLoss\n",
    "\n",
    "# 不再提示pd的SettingWithCopyWarning为错误\n",
    "warnings.simplefilter(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "# 1. 加载股票数据\n",
    "data = pd.read_csv(\"stock_data.csv\")\n",
    "\n",
    "# 2. 数据预处理\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"], format=\"%Y/%m/%d\")\n",
    "data = data.sort_values(\"Date\").reset_index(drop=True)\n",
    "data[\"time_idx\"] = data.index + 1  # 从1开始构造顺序的时间索引\n",
    "data[\"static\"] = data[\"stock_id\"]\n",
    "\n",
    "# 3. 定义采样参数\n",
    "max_encoder_length = 60\n",
    "max_prediction_length = 20\n",
    "\n",
    "# 调整训练截止点，确保验证数据中至少有 max_encoder_length + max_prediction_length 个时间步\n",
    "training_cutoff = data[\"time_idx\"].max() - (max_encoder_length + max_prediction_length)\n",
    "\n",
    "print(f\"数据总长度: {data['time_idx'].max()}, 训练截止点: {training_cutoff}\")\n",
    "\n",
    "# 4. 构造训练数据集\n",
    "training = TimeSeriesDataSet(\n",
    "    data=data[data.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Close\",\n",
    "    group_ids=[\"stock_id\"],\n",
    "    categorical_encoders={\"stock_id\": NaNLabelEncoder().fit(data[\"stock_id\"])},\n",
    "    static_categoricals=[\"static\"],\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_unknown_reals=[\"Close\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    target_normalizer=GroupNormalizer(groups=[\"stock_id\"], transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    randomize_length=None,\n",
    ")\n",
    "\n",
    "# 5. 构造验证数据集\n",
    "# 注意这里的数据要确保有足够的样本构成完整的序列\n",
    "validation_data = data[data.time_idx > training_cutoff]\n",
    "print(f\"验证集数量: {len(validation_data)}\")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training,\n",
    "    validation_data,\n",
    "    stop_randomization=True,\n",
    ")\n",
    "\n",
    "# 6. 创建 DataLoader\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# （后续代码省略：回调、Trainer、模型创建与训练等）"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T08:38:36.362574Z",
     "start_time": "2025-03-23T08:38:31.737324Z"
    }
   },
   "id": "301f2b0316d8a8cb",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "D:\\anaconda3\\envs\\wind\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "D:\\anaconda3\\envs\\wind\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 13.6k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type                   | Params | Mode \n",
      "--------------------------------------------------------------------------\n",
      "0 | loss                   | NormalDistributionLoss | 0      | train\n",
      "1 | logging_metrics        | ModuleList             | 0      | train\n",
      "2 | embeddings             | MultiEmbedding         | 1      | train\n",
      "3 | rnn                    | LSTM                   | 13.6 K | train\n",
      "4 | distribution_projector | Linear                 | 66     | train\n",
      "--------------------------------------------------------------------------\n",
      "13.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.6 K    Total params\n",
      "0.055     Total estimated model params size (MB)\n",
      "12        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b0485e36287405f849a319d53276deb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\wind\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "D:\\anaconda3\\envs\\wind\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "D:\\anaconda3\\envs\\wind\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a39ba66d79744f219fe77e0d162d7de1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8744a88c24714193b968fb348b759b6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7cbb4d8f306b4de59537ed1fdfa98f8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0314fd1ee3724b49b55220f42d6e32a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "200e8ab5b5294103af8f015179b14a4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4158010f2f36441283e90962013776e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2473f43c2a5b439fad44083a13b38b64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec302f67517343dab50dea7b1fafb78f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec7b0dce6c224f2a8807a79a8fad89f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17b419c0906e4986a57a1765340c4618"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf3e2ab33d114227af03c5a1a2040071"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "D:\\anaconda3\\envs\\wind\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error of model: 63.53936767578125\n"
     ]
    }
   ],
   "source": [
    "# 6. 定义回调函数和 Trainer\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=False, mode=\"min\"\n",
    ")\n",
    "lr_logger = LearningRateMonitor()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"gpu\",  # 如果没有GPU，可设为 'cpu'\n",
    "    devices=\"auto\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,\n",
    "    limit_val_batches=3,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    ")\n",
    "\n",
    "# 7. 构建 DeepAR 模型\n",
    "deepar = DeepAR.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.1,\n",
    "    hidden_size=32,\n",
    "    dropout=0.1,\n",
    "    loss=NormalDistributionLoss(),\n",
    "    log_interval=10,\n",
    "    log_val_interval=3,\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {deepar.size() / 1e3:.1f}k\")\n",
    "\n",
    "# 8. 开始训练\n",
    "torch.set_num_threads(10)\n",
    "trainer.fit(\n",
    "    deepar,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "\n",
    "# 9. 验证集预测和误差计算（以平均绝对误差MAE为例）\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "predictions = deepar.predict(val_dataloader)\n",
    "device = predictions.device\n",
    "actuals = actuals.to(device)\n",
    "print(f\"Mean absolute error of model: {(actuals - predictions).abs().mean()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-23T08:22:34.525729Z",
     "start_time": "2025-03-23T08:22:09.838713Z"
    }
   },
   "id": "f3dbc225272ba04a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "23d6b3683083c470"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
